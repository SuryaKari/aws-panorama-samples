{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building And Deploy Edge Optimized Object Detection Model (YoloV5s) To AWS Panorama : Avastus Pipelines\n",
    "\n",
    "This notebook instance will act as the environment for setting up and triggering changes to our pipeline.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import avastus_utils\n",
    "from avastus_utils import communication\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "import os\n",
    "import numpy\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "from yolov5 import utils\n",
    "import glob\n",
    "import joblib\n",
    "import datetime\n",
    "import warnings\n",
    "import ipyplot\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "# from tqdm import tqdm\n",
    "from time import strftime\n",
    "from PIL import Image\n",
    "\n",
    "# Sagemaker Imports\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.debugger import (Rule,\n",
    "                                rule_configs,\n",
    "                                ProfilerConfig, \n",
    "                                FrameworkProfile, \n",
    "                                DetailedProfilingConfig, \n",
    "                                DataloaderProfilingConfig, \n",
    "                                PythonProfilingConfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Step 1:  Ground Truth Manifest to Yolo Format\n",
    "\n",
    "  In the code cell below, we will take a completed Ground Truth Job, download its artifacts like the images and the manifest file, convert it to the Yolo Format. If you are interested in the YoloV5 format, please see this [Link](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Provide Ground Truth Job Name\n",
    "GT_Job_Name = 'wheel-job'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeling_job = avastus_utils.download_from_labeling_job(GT_Job_Name)\n",
    "labeling_job.download_s3_data()\n",
    "class_map = labeling_job.get_class_map()\n",
    "labeling_job.split_images_test_train_valid()\n",
    "labeling_job.create_train_test_valid()\n",
    "lenclass = len(class_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Step 2: Custom Training Container (ONE TIME RUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam_client = boto3.client('iam')\n",
    "role=get_execution_role()\n",
    "base_role_name=role.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ./ecr-repo\n",
    "echo $(pwd)\n",
    "container_name=yolov5-training-sagemaker\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-east-1}\n",
    "\n",
    "echo ${region}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${container_name}:1.10.0-gpu-py38\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${container_name}\" > /dev/null 2>&1\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${container_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin \"763104351884.dkr.ecr.us-west-2.amazonaws.com\")\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "docker build -f Dockerfile -t ${fullname} .\n",
    "# docker tag ${container_name} ${fullname}\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3:  Upload Data to S3 \n",
    "\n",
    "We will utilize this notebook to perform some of the setup that will be required to trigger the first execution of our pipeline.   In this second step in our Machine Learning pipeline, we are going to simulate what would typically be the last step in an Analytics pipeline of creating datasets. \n",
    "\n",
    "To accomplish this, we will actually be uploading data from our local notebook instance (data can be found under /data/1-train/*) to S3.  In a typical scenario, this would be done through your analytics pipeline.  We will use the S3 bucket that was created through the CloudFormation template we launched at the beginning of the lab. You can validate the S3 bucket exists by:\n",
    "  1. Going to the [S3 Service](https://s3.console.aws.amazon.com/s3/) inside the AWS Console\n",
    "  2. Find the name of the S3 data bucket created by the CloudFormation template: mlops-data-*yourintials*-*randomid*\n",
    "  \n",
    "In the code cell below, we'll take a look at the training/test/validation datasets and then upload them to S3. \n",
    "\n",
    "   ### UPDATE THE BUCKET NAME BELOW BEFORE EXECUTING THE CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE THE NAME OF THE BUCKET TO MATCH THE ONE WE CREATED THROUGH THE CLOUDFORMATION TEMPLATE\n",
    "# Example: mlops-data-jdd-df4d4850\n",
    "#bucket = 'mlops-data-<yourinitials>-<generated id>'\n",
    "bucket = 'mlops-data-avastus-8da27a80'\n",
    "\n",
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sqs_client = boto3.client(\"sqs\")\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Creating SQS Communication Channels\n",
    "\n",
    "resp_1 = communication().create_queue('avastus-queue')\n",
    "resp_2 = communication().create_queue('train-lambda-queue')\n",
    "resp_3 = communication().create_queue('convert-onnx-queue')\n",
    "resp_4 = communication().create_queue('optimize-model-queue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEND MESSAGE TO THE SQS AVASTUS-QUEUE. THIS WILL BE BE READ BY THE TRAINING LAMBDA TO GET INFORMATION ABOUT THE S3 BUCKET AND FOLDER NAME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = communication().send_message(\n",
    "            qurl = communication().get_queue_url('avastus-queue'),\n",
    "            message = 'bucket:{}, data_folder:{}, epochs:{}'.format(bucket, GT_Job_Name, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SET CODE AND ARTIFACT PATHS. ALSO UPLOAD DATA TO THE S3 BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_location = f's3://{bucket}/avastus_yolov5/sm_codes'\n",
    "output_path = f's3://{bucket}/avastus_yolov5/output' \n",
    "s3_log_path = f's3://{bucket}/avastus_yolov5/tf_logs'\n",
    "source_code_path = f's3://{bucket}/avastus_yolov5/source_code'\n",
    "s3_data_path = f's3://{bucket}/dataset/{GT_Job_Name}'\n",
    "\n",
    "checkpoint_s3_uri = f's3://{bucket}/avastus_yolov5/checkpoints'\n",
    "!aws s3 sync ./{GT_Job_Name} {s3_data_path} --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Step 3 : Customize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_line_cell_magic\n",
    "\n",
    "@register_line_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "    with open(line, 'w') as f:\n",
    "        f.write(cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate yolov5/data/data_sm.yaml\n",
    "train: /opt/ml/input/data/yolov5_input/train/images\n",
    "val: /opt/ml/input/data/yolov5_input/valid/images\n",
    "\n",
    "nc: {lenclass}\n",
    "names: {class_map}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate yolov5/data_sm.yaml\n",
    "train: /opt/ml/input/data/yolov5_input/train/images\n",
    "val: /opt/ml/input/data/yolov5_input/valid/images\n",
    "\n",
    "nc: {lenclass}\n",
    "names: {class_map}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate data_sm.yaml\n",
    "train: /opt/ml/input/data/yolov5_input/train/images\n",
    "val: /opt/ml/input/data/yolov5_input/valid/images\n",
    "\n",
    "nc: {lenclass}\n",
    "names: {class_map}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"tar -zcvf yolov5.tar.gz ./yolov5\")\n",
    "os.system('aws s3 cp yolov5.tar.gz s3://{}/avastus_yolov5/source_code/yolov5.tar.gz --quiet'.format(bucket))\n",
    "os.system('rm -r yolov5.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4:  Commit Training Code To Trigger Pipeline Build\n",
    "\n",
    "In this step, we are going to trigger an execution of the pipeline by committing our training code to the CodeCommit repository that was setup as part of the CloudFormation stack.  The CodeCommit repository created was associated with this SageMaker Notebook Instance via a setting in the CloudFormation Stack using the [SageMaker Notebook Instance Git Association](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi-git-repo.html) feature.\n",
    "\n",
    "The pipeline is currently setup to trigger on a commit to the master branch; however, this should be adjusted in a real-world scenario based on your branching strategy. \n",
    "\n",
    "The CodeCommit repository created can be viewed by:\n",
    "  1. Going to the [CodeCommit Service](https://console.aws.amazon.com/codesuite/codecommit/repositories) inside the AWS Console\n",
    "  2. Find the name of the repository created by the CloudFormation template: mlops-codecommit-byo-*yourinitials*\n",
    "  \n",
    "**UPDATE** Ensure you update the cell below where noted prior to executing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the CodeCommit repository -\n",
    "# This Git integration was configured as part of the creation of the notebook instance in the CloudFormation stack.\n",
    "\n",
    "# The following will return the CodeCommit repository that has been configured with this notebook and will be used \n",
    "# for the source control repository during this workshop. \n",
    "!rm -r {GT_Job_Name}\n",
    "\n",
    "# Ensure remote repo is setup\n",
    "!git remote -v\n",
    "\n",
    "!git config --global user.name \"Surya Kari\"\n",
    "!git config --global user.email karisury@amazon.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commit training code to the CodeCommit repository to trigger the execution of the CodePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git pull\n",
    "!git add ./model-code/*\n",
    "!git add ./lambda-code/*\n",
    "!git add ./ecr-repo/*\n",
    "!git add ./yolov5\n",
    "!git add *\n",
    "!git commit -m \"Avastus Pipelines : Wheel Labeling Job Test 2 with ONNX: 20 Epochs\"\n",
    "!git push origin master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 4:  Monitor CodePipeline Execution\n",
    "\n",
    "The code above will trigger the execution of your CodePipeline. You can monitor progress of the pipeline execution in the [CodePipeline dashboard](https://console.aws.amazon.com/codesuite/codepipeline/pipelines).\n",
    "\n",
    "You can also validate that your code is now committed to the CodeCommit repository in the [CodeCommit dashboard](https://console.aws.amazon.com/codesuite/codecommit/repositories)\n",
    "\n",
    "As the pipeline is executing information is being logged to [Cloudwatch logs](https://console.aws.amazon.com/cloudwatch/logs).  Explore the logs for your Lambda functions (/aws/lambda/MLOps-BYO*) as well as output logs from SageMaker (/aws/sagemaker/*)\n",
    "\n",
    "\n",
    "Note: It will take awhile to execute all the way through the pipeline.  Please don't proceed to the next step until the last stage is shows **'succeeded'**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we may not want rebuild the training/inference container image in the case where we only want to retrain the model, you could optionally create a separate retraining pipeline that excludes the rebuild of the image.  Depending on what you are using for orchestration across your pipeline, you can accomplish this through a single or multiple pipelines. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing the cell above, you will see a new trigger for pipeline execution when you click on the link to your [pipeline](https://console.aws.amazon.com/codesuite/codepipeline/pipelines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 5:  Output Of Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = communication().receive_message(qurl = communication().get_queue_url('train-lambda-queue'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator.attach(messages[0][0].split(':')[-1].replace('\"',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts_dir = estimator.model_data.replace('model.tar.gz', '')\n",
    "print(artifacts_dir)\n",
    "!aws s3 ls --human-readable {artifacts_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './model'\n",
    "!rm -rf $model_dir\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "!aws s3 cp {artifacts_dir}model.tar.gz {model_dir}/model.tar.gz\n",
    "!tar -xvzf {model_dir}/model.tar.gz -C {model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(f'{os.getcwd()}/yolov5')\n",
    "#display = utils.notebook_init()  # checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for img_path in glob.glob('model/*/*'):\n",
    "    if img_path.split('.')[-1] in ['jpg','png']:\n",
    "        images.append(mpimg.imread(img_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipyplot.plot_images(images, max_images=5, img_width=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
